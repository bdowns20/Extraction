{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfminer for PDF extraction\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "\n",
    "# AzureOpenAI\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Configuration\n",
    "# ---------------------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Functions (Basic Examples)\n",
    "# ---------------------------------------\n",
    "def is_valid_ip(ip):\n",
    "    \"\"\"\n",
    "    Simple regex for IPv4 addresses (e.g., 185.23.76.19).\n",
    "    Adjust/expand for IPv6 if needed.\n",
    "    \"\"\"\n",
    "    pattern = r\"^(25[0-5]|2[0-4]\\d|[01]?\\d?\\d)\\.\" \\\n",
    "              r\"(25[0-5]|2[0-4]\\d|[01]?\\d?\\d)\\.\" \\\n",
    "              r\"(25[0-5]|2[0-4]\\d|[01]?\\d?\\d)\\.\" \\\n",
    "              r\"(25[0-5]|2[0-4]\\d|[01]?\\d?\\d)$\"\n",
    "    if re.match(pattern, ip.strip()):\n",
    "        return True\n",
    "    \n",
    "    # Quick check for IPv6 (basic check for presence of colons).\n",
    "    if \":\" in ip.strip():\n",
    "        return True\n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_email(email):\n",
    "    \"\"\"\n",
    "    Simple regex check for email addresses.\n",
    "    \"\"\"\n",
    "    pattern = r\"^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$\"\n",
    "    return bool(re.match(pattern, email.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_iban(iban):\n",
    "    \"\"\"\n",
    "    Placeholder for IBAN validation. If you install a library\n",
    "    like 'pyIBAN', you could do real checks.\n",
    "    For now, we do a naive length check to illustrate.\n",
    "    \"\"\"\n",
    "    iban = iban.replace(\" \", \"\")  # remove spaces\n",
    "    return 15 <= len(iban) <= 34  # typical IBAN length range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Extraction & Cleaning\n",
    "# ---------------------------------------\n",
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using pdfminer.high_level.extract_text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = pdf_extract_text(pdf_file_path)\n",
    "        logger.info(f\"Extracted {len(text)} characters from PDF.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from {pdf_file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean extracted text by removing extraneous newlines and extra spaces.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    cleaned_text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "# ---------------------------------------\n",
    "def chunk_text(text, max_chars=2000):\n",
    "    \"\"\"\n",
    "    Splits text into smaller chunks to avoid token/character limits.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_chars\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "    logger.info(f\"Divided text into {len(chunks)} chunks.\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Call\n",
    "# ---------------------------------------\n",
    "def call_azure_openai_api(prompt, openai_client, model_name=\"gpt35-turbo-16k\"):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Azure OpenAI using the chat completions endpoint.\n",
    "    Returns the assistant's response text.\n",
    "    \"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI that extracts and normalizes entities/relationships from text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    if response.choices:\n",
    "        # 'message' is a ChatCompletionMessage object; use .content\n",
    "        return response.choices[0].message.content.strip()\n",
    "    else:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship Extraction\n",
    "# ---------------------------------------\n",
    "def extract_relationships_from_chunk(text_chunk, openai_client):\n",
    "    \"\"\"\n",
    "    Extract relationships among entities from a text chunk.\n",
    "    The prompt instructs the model to use standardized relationship labels\n",
    "    and omit meaningless 'Unknown' relationships.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analyze the following text and extract relationships between the entities mentioned.\n",
    "Use these standard short forms for the 'relation' field where possible:\n",
    "- HAS_EMAIL\n",
    "- HAS_IP\n",
    "- IS_ASSOCIATED_WITH\n",
    "- IS_REGISTERED_AS\n",
    "- HAS_WEBSITE\n",
    "(You can add or choose from others, but keep them short.)\n",
    "\n",
    "If a relationship is purely \"Unknown - Unknown\" or provides no meaningful information,\n",
    "omit it from the output.\n",
    "\n",
    "Return the results as valid JSON in this format:\n",
    "{{\n",
    "  \"relationships\": [\n",
    "    {{\n",
    "      \"source\": \"Entity Name\",\n",
    "      \"relation\": \"Short Relationship Label (e.g. HAS_EMAIL, HAS_IP)\",\n",
    "      \"target\": \"Related Entity or Value\",\n",
    "      \"details\": \"Additional details if available (optional)\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Text to analyze:\n",
    "{text_chunk}\n",
    "    \"\"\"\n",
    "    logger.debug(\"Extracting relationships from chunk...\")\n",
    "    response_text = call_azure_openai_api(prompt, openai_client)\n",
    "    logger.debug(f\"Raw API response for relationships: {response_text}\")\n",
    "    try:\n",
    "        relationships = json.loads(response_text)\n",
    "        return relationships.get(\"relationships\", [])\n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        logger.error(f\"Failed to parse JSON for relationships: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_relationships(chunks, openai_client):\n",
    "    \"\"\"\n",
    "    Aggregate relationships from all chunks, ensuring uniqueness and omitting\n",
    "    meaningless or duplicate entries.\n",
    "    \"\"\"\n",
    "    relationships_list = []\n",
    "    seen = set()\n",
    "\n",
    "    for chunk in chunks:\n",
    "        rels = extract_relationships_from_chunk(chunk, openai_client)\n",
    "        for rel in rels:\n",
    "            # Create a tuple key to check uniqueness.\n",
    "            key = (\n",
    "                rel.get(\"source\", \"\"),\n",
    "                rel.get(\"relation\", \"\"),\n",
    "                rel.get(\"target\", \"\"),\n",
    "                rel.get(\"details\", \"\")\n",
    "            )\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                relationships_list.append(rel)\n",
    "\n",
    "    return relationships_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Extraction & Validation\n",
    "# ---------------------------------------\n",
    "def extract_entities_from_chunk(text_chunk, openai_client):\n",
    "    \"\"\"\n",
    "    Extract entities (IP addresses, emails, company names, person names, IBANs)\n",
    "    from a text chunk using the Azure OpenAI API.\n",
    "    Then validate each entity to discard malformed data.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Extract the following entities from the text below:\n",
    "- IP addresses\n",
    "- Emails\n",
    "- Company names\n",
    "- Person names\n",
    "- IBANs\n",
    "\n",
    "Return them as valid JSON in the format:\n",
    "{{\n",
    "    \"ip_addresses\": [],\n",
    "    \"emails\": [],\n",
    "    \"company_names\": [],\n",
    "    \"person_names\": [],\n",
    "    \"ibans\": []\n",
    "}}\n",
    "\n",
    "If you see no valid data for a category, leave that list empty.\n",
    "\n",
    "Text to analyze:\n",
    "{text_chunk}\n",
    "    \"\"\"\n",
    "    logger.debug(\"Extracting entities from chunk...\")\n",
    "    response_text = call_azure_openai_api(prompt, openai_client)\n",
    "    logger.debug(f\"Raw API response for entities: {response_text}\")\n",
    "    try:\n",
    "        entities = json.loads(response_text)\n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        logger.error(f\"Failed to parse JSON for entities: {e}\")\n",
    "        entities = {}\n",
    "\n",
    "    # Fill missing keys with empty lists\n",
    "    ip_addresses = entities.get(\"ip_addresses\", [])\n",
    "    emails       = entities.get(\"emails\", [])\n",
    "    companies    = entities.get(\"company_names\", [])\n",
    "    persons      = entities.get(\"person_names\", [])\n",
    "    ibans        = entities.get(\"ibans\", [])\n",
    "\n",
    "    # Validate/normalize\n",
    "    ip_addresses = [ip for ip in ip_addresses if is_valid_ip(ip)]\n",
    "    emails       = [em for em in emails if is_valid_email(em)]\n",
    "    ibans        = [iban for iban in ibans if is_valid_iban(iban)]\n",
    "    # Company and person names are not syntactically validated,\n",
    "    # but can be deduplicated/merged later.\n",
    "\n",
    "    return {\n",
    "        \"ip_addresses\": ip_addresses,\n",
    "        \"emails\": emails,\n",
    "        \"company_names\": companies,\n",
    "        \"person_names\": persons,\n",
    "        \"ibans\": ibans\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Substring-Based Deduplication\n",
    "# ---------------------------------------\n",
    "def unify_substring_entities(entity_set):\n",
    "    \"\"\"\n",
    "    Given a set of entity strings, unify duplicates if one is entirely contained in another.\n",
    "    Keeps the longer name if the shorter is a substring of the longer.\n",
    "\n",
    "    Example: 'Crimson Viper' in 'Crimson Viper Group' => keep 'Crimson Viper Group'.\n",
    "    \"\"\"\n",
    "    entities = list(entity_set)\n",
    "    # Sort by length descending so that longer entities come first\n",
    "    # and can subsume shorter ones.\n",
    "    entities.sort(key=len, reverse=True)\n",
    "\n",
    "    final_list = []\n",
    "    for candidate in entities:\n",
    "        skip = False\n",
    "        for i, existing in enumerate(final_list):\n",
    "            # If the candidate is contained in an existing entity, skip candidate\n",
    "            if candidate.lower() in existing.lower():\n",
    "                skip = True\n",
    "                break\n",
    "            # If an existing entity is contained in the candidate, unify to candidate\n",
    "            if existing.lower() in candidate.lower():\n",
    "                final_list[i] = candidate\n",
    "                skip = True\n",
    "                break\n",
    "        if not skip:\n",
    "            final_list.append(candidate)\n",
    "\n",
    "    return set(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_entities(chunks, openai_client):\n",
    "    \"\"\"\n",
    "    Aggregate entities from all chunks into unique sets and then\n",
    "    deduplicate near-duplicates for company and person names.\n",
    "    \"\"\"\n",
    "    aggregated = {\n",
    "        \"ip_addresses\": set(),\n",
    "        \"emails\": set(),\n",
    "        \"company_names\": set(),\n",
    "        \"person_names\": set(),\n",
    "        \"ibans\": set()\n",
    "    }\n",
    "\n",
    "    for chunk in chunks:\n",
    "        entities = extract_entities_from_chunk(chunk, openai_client)\n",
    "        aggregated[\"ip_addresses\"].update(entities[\"ip_addresses\"])\n",
    "        aggregated[\"emails\"].update(entities[\"emails\"])\n",
    "        aggregated[\"company_names\"].update(entities[\"company_names\"])\n",
    "        aggregated[\"person_names\"].update(entities[\"person_names\"])\n",
    "        aggregated[\"ibans\"].update(entities[\"ibans\"])\n",
    "\n",
    "    # Deduplicate company and person names\n",
    "    aggregated[\"company_names\"] = unify_substring_entities(aggregated[\"company_names\"])\n",
    "    aggregated[\"person_names\"]  = unify_substring_entities(aggregated[\"person_names\"])\n",
    "\n",
    "    # Convert sets to sorted lists for readability.\n",
    "    return {key: sorted(value) for key, value in aggregated.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Saving (with Encoding Fix)\n",
    "# ---------------------------------------\n",
    "def save_entities_to_csv(entities, csv_file_path):\n",
    "    \"\"\"\n",
    "    Save extracted entities to a CSV file with columns: 'entity_type' and 'value',\n",
    "    applying encoding fixes as needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(csv_file_path, mode=\"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"entity_type\", \"value\"])\n",
    "\n",
    "            for entity_type, values in entities.items():\n",
    "                for val in values:\n",
    "                    # Apply encoding fix\n",
    "                    val_fixed = val.encode('latin-1', errors='replace').decode('utf-8', errors='replace')\n",
    "                    writer.writerow([entity_type, val_fixed])\n",
    "\n",
    "        logger.info(f\"Entities saved to {csv_file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving entities to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_relationships_to_csv(relationships, csv_file_path):\n",
    "    \"\"\"\n",
    "    Save extracted relationships to a CSV file with columns:\n",
    "    'source', 'relation', 'target', 'details',\n",
    "    applying encoding fixes as needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(csv_file_path, mode=\"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"source\", \"relation\", \"target\", \"details\"])\n",
    "\n",
    "            for rel in relationships:\n",
    "                source   = rel.get(\"source\", \"\")\n",
    "                relation = rel.get(\"relation\", \"\")\n",
    "                target   = rel.get(\"target\", \"\")\n",
    "                details  = rel.get(\"details\", \"\")\n",
    "\n",
    "                # Apply encoding fix to each field\n",
    "                source_fixed   = source.encode('latin-1', errors='replace').decode('utf-8', errors='replace')\n",
    "                relation_fixed = relation.encode('latin-1', errors='replace').decode('utf-8', errors='replace')\n",
    "                target_fixed   = target.encode('latin-1', errors='replace').decode('utf-8', errors='replace')\n",
    "                details_fixed  = details.encode('latin-1', errors='replace').decode('utf-8', errors='replace')\n",
    "\n",
    "                writer.writerow([source_fixed, relation_fixed, target_fixed, details_fixed])\n",
    "\n",
    "        logger.info(f\"Relationships saved to {csv_file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving relationships to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Pipeline\n",
    "# ---------------------------------------\n",
    "def extract_data_from_pdf(pdf_file_path, openai_client, max_chars=1500):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline:\n",
    "      1. Extract text from PDF.\n",
    "      2. Clean and chunk the text.\n",
    "      3. Extract entities and relationships from each chunk.\n",
    "      4. Aggregate, deduplicate, and return the results.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting PDF extraction process...\")\n",
    "    raw_text = extract_text_from_pdf(pdf_file_path)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    \n",
    "    if not cleaned_text:\n",
    "        logger.error(\"No text extracted from PDF.\")\n",
    "        return {}, []\n",
    "\n",
    "    chunks = chunk_text(cleaned_text, max_chars=max_chars)\n",
    "    entities = aggregate_entities(chunks, openai_client)\n",
    "    relationships = aggregate_relationships(chunks, openai_client)\n",
    "    \n",
    "    return entities, relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting PDF extraction process...\n",
      "INFO:__main__:Extracted 8839 characters from PDF.\n",
      "INFO:__main__:Divided text into 6 chunks.\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://aoai.apim.mitre.org/api-key/openai/deployments/gpt35-turbo-16k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Extracted Entities:\n",
      "INFO:__main__:{\n",
      "  \"ip_addresses\": [\n",
      "    \"185.23.76.19\",\n",
      "    \"202.57.63.110\",\n",
      "    \"2a00:1450:400a:801::200e\",\n",
      "    \"45.71.198.12\",\n",
      "    \"81.91.143.7\"\n",
      "  ],\n",
      "  \"emails\": [\n",
      "    \"XXX@bad.com\",\n",
      "    \"benjamin.lee35@randomsite.com\",\n",
      "    \"budlightyum@yahoo.com\",\n",
      "    \"carl.james63@example.com\",\n",
      "    \"cryptoman@yahoo.com\",\n",
      "    \"info@aqua-terra.fake\",\n",
      "    \"internetstorm@gmail.com\",\n",
      "    \"jimmyyo@yahoo.com\",\n",
      "    \"johnsmith@gmail.com\",\n",
      "    \"millertime@gmail.com\",\n",
      "    \"millertime@yahoo.com\",\n",
      "    \"ryan.hicks24@fakemail.org\",\n",
      "    \"scamman@yahoo.com\",\n",
      "    \"yourmom@yahoo.com\",\n",
      "    \"zoe.kelly07@samplemail.net\"\n",
      "  ],\n",
      "  \"company_names\": [\n",
      "    \"Aqua Terra Manufacturing\",\n",
      "    \"Banco de Bogot\\u00e1\",\n",
      "    \"Coastal Finances\",\n",
      "    \"Crimson Viper Group\",\n",
      "    \"Defense Research & Analysis Group (DRAG)\",\n",
      "    \"EuroOne Finance\",\n",
      "    \"Falcon Systems Inc.\",\n",
      "    \"Global Tech Solutions\",\n",
      "    \"NewLondon Bank\",\n",
      "    \"Texas Federal Bank\",\n",
      "    \"Wolf Syndicate\"\n",
      "  ],\n",
      "  \"person_names\": [\n",
      "    \"John \\u201cSpecter\\u201d Whiting\",\n",
      "    \"Maria Gomez\"\n",
      "  ],\n",
      "  \"ibans\": [\n",
      "    \"CO76 1234 5678 9123 4567 89\",\n",
      "    \"DE89 3704 0044 0532 0130 00\",\n",
      "    \"FR76 3000 6000 0102 3456 7890 189\",\n",
      "    \"GB76 LOYD 3090 2212 3456 78\",\n",
      "    \"US12 1234 5678 9012 3456 78\"\n",
      "  ]\n",
      "}\n",
      "INFO:__main__:Extracted Relationships:\n",
      "INFO:__main__:[\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"johnsmith@gmail.com\",\n",
      "    \"details\": \"Primary Contact Email\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Wolf Syndicate\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"yourmom@yahoo.com\",\n",
      "    \"details\": \"Primary Contact Email\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Falcon Systems Inc.\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"jimmyyo@yahoo.com\",\n",
      "    \"details\": \"Public Relations Email\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Global Tech Solutions\",\n",
      "    \"relation\": \"IS_REGISTERED_AS\",\n",
      "    \"target\": \"Technology reseller\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Global Tech Solutions\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"info@aqua-terra.fake\",\n",
      "    \"details\": \"Corporate Email\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"John \\u201cSpecter\\u201d Whiting\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"carl.james63@example.com\",\n",
      "    \"details\": \"Sales/Inquiry Email\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"John \\u201cSpecter\\u201d Whiting\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"Crimson Viper\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Maria Gomez\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"zoe.kelly07@samplemail.net\",\n",
      "    \"details\": \"Personal Alias Email\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Maria Gomez\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"Wolf Syndicate\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Maria Gomez\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"ryan.hicks24@fakemail.org\",\n",
      "    \"details\": \"Personal/Business Email\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"John \\u201cSpecter\\u201d Whiting\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"185.23.76.19\",\n",
      "    \"details\": \"Known IP Usage\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"iple onshore accounts\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"email@example.com\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"iple onshore accounts\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"+44 7890 123456 (UK phone number used for WhatsApp)\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Falcon Systems Inc.\",\n",
      "    \"relation\": \"HAS_WEBSITE\",\n",
      "    \"target\": \"http://www.falcon-sys.fake\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Falcon Systems Inc.\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"XXX@bad.com\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Aqua Terra Manufacturing\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"info@aqua-terra.fake\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Global Tech Solutions\",\n",
      "    \"relation\": \"HAS_WEBSITE\",\n",
      "    \"target\": \"http://www.globaltech.fake\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Global Tech Solutions\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"scamman@yahoo.com\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"John \\u201cSpecter\\u201d Whiting\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"185.23.76.19\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"John \\u201cSpecter\\u201d Whiting\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"millertime@gmail.com\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"45.71.198.12\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"internetstorm@gmail.com\",\n",
      "    \"details\": \"Command & control server suspected\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Wolf Syndicate\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"202.57.63.110\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Wolf Syndicate\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"millertime@yahoo.com\",\n",
      "    \"details\": \"Used for logistical planning in East Asia\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Maria Gomez\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"81.91.143.7\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Maria Gomez\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"budlightyum@yahoo.com\",\n",
      "    \"details\": \"Detected in financial transaction traces\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Global Tech Solutions\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"2a00:1450:400a:801::200e\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Global Tech Solutions\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"cryptoman@yahoo.com\",\n",
      "    \"details\": \"Possibly used for large file transfers\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"Coastal Finances\",\n",
      "    \"details\": \"Large transfers for 'Equipment'\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Wolf Syndicate\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"NewLondon Bank\",\n",
      "    \"details\": \"Suspected arms procurement channel\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Maria Gomez\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"Banco de Bogot\\u00e1\",\n",
      "    \"details\": \"Frequent cross-border transactions\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Falcon Systems Inc.\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"Texas Federal Bank\",\n",
      "    \"details\": \"Official corporate account\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Global Tech Solutions\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"EuroOne Finance\",\n",
      "    \"details\": \"Shell account with minimal activity\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"Wolf Syndicate\",\n",
      "    \"details\": \"Cooperation with Wolf Syndicate further broadens their logistics and financial reach.\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"Multiple IP addresses\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"HAS_EMAIL\",\n",
      "    \"target\": \"Multiple email addresses\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper Group\",\n",
      "    \"relation\": \"IS_REGISTERED_AS\",\n",
      "    \"target\": \"Shell companies\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Shell companies\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"Crimson Viper Group\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper\",\n",
      "    \"relation\": \"HAS_IP\",\n",
      "    \"target\": \"IP addresses\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Crimson Viper\",\n",
      "    \"relation\": \"HAS_WEBSITE\",\n",
      "    \"target\": \"domain registrations\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Falcon Systems Inc.\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"shipping containers\"\n",
      "  },\n",
      "  {\n",
      "    \"source\": \"Aqua Terra Manufacturing\",\n",
      "    \"relation\": \"IS_ASSOCIATED_WITH\",\n",
      "    \"target\": \"shipping containers\"\n",
      "  }\n",
      "]\n",
      "INFO:__main__:Entities saved to extracted_entities.csv\n",
      "INFO:__main__:Relationships saved to extracted_relationships.csv\n"
     ]
    }
   ],
   "source": [
    "# Usage Example (if needed)\n",
    "# ---------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Instantiate the AzureOpenAI client with your credentials.\n",
    "    openai_client = AzureOpenAI(\n",
    "        azure_endpoint=\"https://aoai.apim.mitre.org/api-key\",   # Replace with your endpoint\n",
    "        api_key=\"9abc905da5104e8eb8d6ec3ceb27f767\",              # Replace with your API key\n",
    "        default_headers={\"Content-Type\": \"application/json\"},\n",
    "        api_version=\"2023-03-15-preview\",\n",
    "    )\n",
    "\n",
    "    # Path to your PDF file\n",
    "    pdf_file_path = \"/Users/bdowns/tflima/FakePDF.pdf\"\n",
    "\n",
    "    # Run the extraction pipeline\n",
    "    entities, relationships = extract_data_from_pdf(pdf_file_path, openai_client, max_chars=1500)\n",
    "\n",
    "    # Log and display results\n",
    "    logger.info(\"Extracted Entities:\")\n",
    "    logger.info(json.dumps(entities, indent=2))\n",
    "    \n",
    "    logger.info(\"Extracted Relationships:\")\n",
    "    logger.info(json.dumps(relationships, indent=2))\n",
    "\n",
    "    # Save results to CSV files (with encoding fixes + dedup)\n",
    "    save_entities_to_csv(entities, \"extracted_entities.csv\")\n",
    "    save_relationships_to_csv(relationships, \"extracted_relationships.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflima_3.12.0_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
